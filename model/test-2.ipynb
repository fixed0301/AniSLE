{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0042bb13-de0c-4d80-80f4-ae41da1b5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "# SAM (segment-anything)\n",
    "# from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# Diffusers / Hugging Face\n",
    "import torch\n",
    "# from diffusers import (\n",
    "#     StableDiffusionControlNetInpaintPipeline,\n",
    "#     ControlNetModel,\n",
    "#     UniPCMultistepScheduler,\n",
    "# )\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233940de-c2bd-4df1-8f43-6326f2b69cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path: str) -> np.ndarray:\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_image(img: np.ndarray, path: str):\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(path, img_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8369daec-0c7f-4620-b7d6-fb5a73c6fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_sam(model_type: str = \"vit_h\", checkpoint: str = None, device=\"cuda\"):\n",
    "    from segment_anything import SamPredictor, sam_model_registry\n",
    "    if checkpoint is None:\n",
    "        raise ValueError(\"Please provide a SAM checkpoint path or set checkpoint argument.\")\n",
    "    sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "    sam.to(device=device)\n",
    "    predictor = SamPredictor(sam)\n",
    "    return predictor\n",
    "\n",
    "\n",
    "def get_sam_masks(predictor, image: np.ndarray, grid_size: int = 32) -> List[np.ndarray]:\n",
    "    image_torch = image.astype(np.uint8)\n",
    "    predictor.set_image(image_torch)\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "    masks = []\n",
    "\n",
    "    ys = np.linspace(0, h - 1, grid_size, dtype=int)\n",
    "    xs = np.linspace(0, w - 1, grid_size, dtype=int)\n",
    "\n",
    "    points = [[x, y] for y in ys for x in xs]\n",
    "    points = np.array(points)\n",
    "\n",
    "    chunk = 512\n",
    "    for i in range(0, len(points), chunk):\n",
    "        pts = points[i:i + chunk]\n",
    "        input_points = pts\n",
    "        input_labels = np.ones(len(pts), dtype=int)\n",
    "        masks_out, scores, logits = predictor.predict(\n",
    "            point_coords=input_points,\n",
    "            point_labels=input_labels,\n",
    "            multimask_output=True,\n",
    "        )\n",
    "        for mset in masks_out:\n",
    "            for m in mset:\n",
    "                mask = m.astype(bool)\n",
    "                if mask.sum() < 0.001 * h * w:\n",
    "                    continue\n",
    "                masks.append(mask)\n",
    "\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for m in masks:\n",
    "        ys, xs = np.where(m)\n",
    "        if len(xs) == 0:\n",
    "            continue\n",
    "        bb = (min(xs), min(ys), max(xs), max(ys))\n",
    "        if bb in seen:\n",
    "            continue\n",
    "        seen.add(bb)\n",
    "        unique.append(m)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def select_mask_by_pose(masks: List[np.ndarray], bbox: Tuple[int, int, int, int]) -> np.ndarray:\n",
    "    x0, y0, x1, y1 = bbox\n",
    "    h = masks[0].shape[0]\n",
    "    w = masks[0].shape[1]\n",
    "    pose_mask = np.zeros((h, w), dtype=bool)\n",
    "    pose_mask[y0:y1, x0:x1] = True\n",
    "\n",
    "    best_mask, best_iou = None, 0.0\n",
    "    for m in masks:\n",
    "        inter = np.logical_and(m, pose_mask).sum()\n",
    "        union = np.logical_or(m, pose_mask).sum()\n",
    "        if union == 0:\n",
    "            continue\n",
    "        iou = inter / union\n",
    "        if iou > best_iou:\n",
    "            best_iou, best_mask = iou, m\n",
    "    if best_mask is None and len(masks) > 0:\n",
    "        best_mask = max(masks, key=lambda x: x.sum())\n",
    "    return best_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d42d67b-a9d8-4747-8e45-8d965e73a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Pose JSON utilities & mask generation ---------------------------\n",
    "\n",
    "def load_pose_json(path: str) -> dict:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def extract_keypoints_from_dict(pose_dict: dict, image_size: Tuple[int,int]) -> list:\n",
    "    \"\"\"Unified extractor for COCO-like, DWpose-like, or aligned (pixel) coords.\"\"\"\n",
    "    H, W = image_size\n",
    "\n",
    "    # DWpose style: {\"bodies\": [[[x,y],...]]} OR {\"bodies\": [[x,y], ...]}\n",
    "    if 'bodies' in pose_dict and len(pose_dict['bodies']) > 0:\n",
    "        arr = pose_dict['bodies'][0]\n",
    "        # case 1: each element is [x,y]\n",
    "        if all(isinstance(pt, list) and len(pt) == 2 for pt in arr):\n",
    "            kps = [(pt[0], pt[1], 1.0) for pt in arr]\n",
    "            return kps\n",
    "        # case 2: flat list [x1,y1, x2,y2, ...]\n",
    "        if all(isinstance(val, (int,float)) for val in arr):\n",
    "            kps = [(arr[i], arr[i+1], 1.0) for i in range(0, len(arr), 2)]\n",
    "            return kps\n",
    "\n",
    "    # Aligned pose: [[x,y], [x,y], ...]\n",
    "    if isinstance(pose_dict, list) and all(isinstance(pt, list) and len(pt)==2 for pt in pose_dict):\n",
    "        return [(x/W, y/H, 1.0) for x,y in pose_dict]\n",
    "\n",
    "    # direct x,y arrays\n",
    "    if 'x' in pose_dict and 'y' in pose_dict:\n",
    "        xs = pose_dict['x']; ys = pose_dict['y']; vs = pose_dict.get('v', [1.0]*len(xs))\n",
    "        return list(zip(xs, ys, vs))\n",
    "\n",
    "    # fallback\n",
    "    for v in pose_dict.values():\n",
    "        if isinstance(v, list) and len(v) % 3 == 0 and len(v) >= 3:\n",
    "            arr = v\n",
    "            return [(arr[i], arr[i+1], arr[i+2]) for i in range(0, len(arr), 3)]\n",
    "    return []\n",
    "    \n",
    "def pose_jsons_to_mask(orig_pose: dict, aligned_pose: dict, image_size: Tuple[int,int], thresh: float=0.05) -> np.ndarray:\n",
    "    \"\"\"Generate mask from differences between two pose keypoint sets.\"\"\"\n",
    "    H, W = image_size\n",
    "    mask = np.zeros((H, W), dtype=bool)\n",
    "\n",
    "    kps0, kps1 = orig_pose['kps'], aligned_pose['kps']\n",
    "    for (x0,y0,v0), (x1,y1,v1) in zip(kps0, kps1):\n",
    "        if v0 < 0.3 or v1 < 0.3:\n",
    "            continue\n",
    "        dx, dy = abs(x0-x1), abs(y0-y1)\n",
    "        if dx > thresh or dy > thresh:\n",
    "            cx, cy = int(x1*W), int(y1*H)\n",
    "            cv2.circle(mask, (cx,cy), 15, 1, -1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08687f11-6de2-48c6-ba8d-33b0736ead9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Example main flow ---------------------------\n",
    "\n",
    "def main_example(image_path: str, orig_pose_json: str, aligned_pose_json: str, sam_checkpoint: str = None, hf_token: str = None):\n",
    "    if hf_token:\n",
    "        os.environ['HF_TOKEN'] = hf_token\n",
    "\n",
    "    img_np = load_image(image_path)\n",
    "    H, W = img_np.shape[:2]\n",
    "\n",
    "    sam_predictor = None\n",
    "    if sam_checkpoint is not None:\n",
    "        sam_predictor = init_sam(model_type='vit_h', checkpoint=sam_checkpoint, device='cuda')\n",
    "\n",
    "    orig = load_pose_json(orig_pose_json)\n",
    "    aligned = load_pose_json(aligned_pose_json)\n",
    "\n",
    "    kps_orig = extract_keypoints_from_dict(orig, (H,W))\n",
    "    kps_aligned = extract_keypoints_from_dict(aligned, (H,W))\n",
    "\n",
    "    mask_bool = pose_jsons_to_mask({'kps': kps_orig}, {'kps': kps_aligned}, (H,W))\n",
    "    mask_np = (mask_bool.astype(np.uint8) * 255)\n",
    "\n",
    "    pose_map = render_pose_map(kps_aligned, (W, H))\n",
    "\n",
    "    prompt = \"photo of a person, realistic clothing and anatomical correctness\"\n",
    "    inpaint_result = run_controlnet_inpaint(Image.fromarray(img_np), mask_np, pose_map, prompt)\n",
    "\n",
    "    out_path = os.path.splitext(image_path)[0] + '_inpainted.png'\n",
    "    inpaint_result.save(out_path)\n",
    "    print('Saved inpainted to', out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "046d313f-d8f2-4822-84e7-befa3f9d54d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'render_pose_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m aligned_pose_json = \u001b[33m\"\u001b[39m\u001b[33mkeypoints_aligned.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m sam_checkpoint = \u001b[33m\"\u001b[39m\u001b[33mcheckpoints/sam_vit_h_4b8939.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmain_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_pose_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned_pose_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mmain_example\u001b[39m\u001b[34m(image_path, orig_pose_json, aligned_pose_json, sam_checkpoint, hf_token)\u001b[39m\n\u001b[32m     20\u001b[39m mask_bool = pose_jsons_to_mask({\u001b[33m'\u001b[39m\u001b[33mkps\u001b[39m\u001b[33m'\u001b[39m: kps_orig}, {\u001b[33m'\u001b[39m\u001b[33mkps\u001b[39m\u001b[33m'\u001b[39m: kps_aligned}, (H,W))\n\u001b[32m     21\u001b[39m mask_np = (mask_bool.astype(np.uint8) * \u001b[32m255\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m pose_map = \u001b[43mrender_pose_map\u001b[49m(kps_aligned, (W, H))\n\u001b[32m     25\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mphoto of a person, realistic clothing and anatomical correctness\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m inpaint_result = run_controlnet_inpaint(Image.fromarray(img_np), mask_np, pose_map, prompt)\n",
      "\u001b[31mNameError\u001b[39m: name 'render_pose_map' is not defined"
     ]
    }
   ],
   "source": [
    "image_path = \"../flask_app/static/uploads/groundimg.jpeg\"\n",
    "orig_pose_json = \"keypoints.json\"\n",
    "aligned_pose_json = \"keypoints_aligned.json\"\n",
    "sam_checkpoint = \"checkpoints/sam_vit_h_4b8939.pth\"\n",
    "\n",
    "main_example(image_path, orig_pose_json, aligned_pose_json, sam_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486dfa3f-1fa2-4a5b-abb2-aa2ed11e11c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
